#!/usr/bin/env python3
"""
GPUä¼˜åŒ–é…ç½®è„šæœ¬
è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®æœ€ä½³GPUè®¾ç½®
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

def check_gpu_capability():
    """æ£€æŸ¥GPUèƒ½åŠ›"""
    try:
        import torch
        
        if not torch.cuda.is_available():
            return None
            
        device_props = torch.cuda.get_device_properties(0)
        gpu_name = device_props.name
        total_memory = device_props.total_memory / 1024**3
        
        return {
            "name": gpu_name,
            "memory": total_memory,
            "compute_capability": f"{device_props.major}.{device_props.minor}"
        }
    except:
        return None

def get_optimal_settings(gpu_info):
    """æ ¹æ®GPUä¿¡æ¯è·å–æœ€ä½³è®¾ç½®"""
    if not gpu_info:
        return {
            "ai_device": "cpu",
            "ai_torch_dtype": "float32",
            "ai_enable_model_offload": False,
            "batch_size": 1,
            "attention_slicing": True
        }
    
    memory_gb = gpu_info["memory"]
    
    if memory_gb >= 12:  # é«˜ç«¯GPU (RTX 4080/4090, RTX 3080Tiç­‰)
        return {
            "ai_device": "cuda",
            "ai_torch_dtype": "float16",
            "ai_enable_model_offload": False,  # æ˜¾å­˜å……è¶³ï¼Œä¸éœ€è¦CPUå¸è½½
            "batch_size": 2,
            "attention_slicing": False,
            "enable_xformers": True,
            "estimated_speed": "3-8ç§’/å¼ "
        }
    elif memory_gb >= 8:  # ä¸­ç«¯GPU (RTX 4070, RTX 3070ç­‰)
        return {
            "ai_device": "cuda", 
            "ai_torch_dtype": "float16",
            "ai_enable_model_offload": True,
            "batch_size": 1,
            "attention_slicing": True,
            "enable_xformers": True,
            "estimated_speed": "5-15ç§’/å¼ "
        }
    elif memory_gb >= 6:  # å…¥é—¨GPU (RTX 4060, RTX 3060ç­‰)
        return {
            "ai_device": "cuda",
            "ai_torch_dtype": "float16", 
            "ai_enable_model_offload": True,
            "batch_size": 1,
            "attention_slicing": True,
            "enable_xformers": False,
            "estimated_speed": "8-20ç§’/å¼ "
        }
    else:  # ä½ç«¯GPU
        return {
            "ai_device": "cuda",
            "ai_torch_dtype": "float16",
            "ai_enable_model_offload": True,
            "batch_size": 1,
            "attention_slicing": True,
            "enable_xformers": False,
            "estimated_speed": "15-30ç§’/å¼ "
        }

def apply_gpu_optimizations():
    """åº”ç”¨GPUä¼˜åŒ–"""
    try:
        # æ£€æŸ¥GPU
        gpu_info = check_gpu_capability()
        settings = get_optimal_settings(gpu_info)
        
        print("=== GPUä¼˜åŒ–é…ç½® ===")
        if gpu_info:
            print(f"æ£€æµ‹åˆ°GPU: {gpu_info['name']}")
            print(f"æ˜¾å­˜: {gpu_info['memory']:.1f} GB")
            print(f"è®¡ç®—èƒ½åŠ›: {settings.get('compute_capability', 'N/A')}")
        else:
            print("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")
        
        print(f"\næ¨èé…ç½®:")
        for key, value in settings.items():
            print(f"  {key}: {value}")
        
        # æµ‹è¯•GPUæ€§èƒ½
        if gpu_info and settings["ai_device"] == "cuda":
            print(f"\n=== GPUæ€§èƒ½æµ‹è¯• ===")
            test_gpu_performance()
        
        return settings
        
    except Exception as e:
        print(f"ä¼˜åŒ–é…ç½®å¤±è´¥: {str(e)}")
        return None

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    try:
        import torch
        import time
        
        device = torch.device("cuda")
        
        # æµ‹è¯•çŸ©é˜µè¿ç®—æ€§èƒ½
        print("æµ‹è¯•GPUè®¡ç®—æ€§èƒ½...")
        start_time = time.time()
        
        for _ in range(10):
            a = torch.randn(2048, 2048, device=device, dtype=torch.float16)
            b = torch.randn(2048, 2048, device=device, dtype=torch.float16)
            c = torch.mm(a, b)
            torch.cuda.synchronize()
        
        elapsed = time.time() - start_time
        print(f"çŸ©é˜µè¿ç®—æµ‹è¯•: {elapsed:.2f}ç§’")
        
        # æµ‹è¯•æ˜¾å­˜ä½¿ç”¨
        print("æµ‹è¯•æ˜¾å­˜ä½¿ç”¨...")
        torch.cuda.empty_cache()
        memory_before = torch.cuda.memory_allocated() / 1024**3
        
        # æ¨¡æ‹ŸSDæ¨¡å‹æ˜¾å­˜ä½¿ç”¨
        dummy_tensors = []
        for i in range(5):
            tensor = torch.randn(512, 512, 512, device=device, dtype=torch.float16)
            dummy_tensors.append(tensor)
        
        memory_after = torch.cuda.memory_allocated() / 1024**3
        memory_used = memory_after - memory_before
        
        print(f"æ¨¡æ‹Ÿæ˜¾å­˜ä½¿ç”¨: {memory_used:.2f} GB")
        
        # æ¸…ç†
        del dummy_tensors
        torch.cuda.empty_cache()
        
        print("âœ“ GPUæ€§èƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"GPUæ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")

def restart_service_with_gpu():
    """é‡å¯æœåŠ¡ä»¥åº”ç”¨GPUé…ç½®"""
    print("\n=== é‡å¯AIæœåŠ¡ ===")
    print("è¯·é‡å¯åç«¯æœåŠ¡ä»¥åº”ç”¨GPUé…ç½®:")
    print("1. åœæ­¢å½“å‰è¿è¡Œçš„åç«¯æœåŠ¡")
    print("2. é‡æ–°è¿è¡Œ: cd backend && python -m app.main")
    print("3. æˆ–è€…è°ƒç”¨APIé‡æ–°åˆå§‹åŒ–: POST /api/ai/initialize")

def main():
    """ä¸»å‡½æ•°"""
    print("=== Stable Diffusion GPUä¼˜åŒ–å·¥å…· ===\n")
    
    settings = apply_gpu_optimizations()
    
    if settings and settings["ai_device"] == "cuda":
        print(f"\nğŸš€ GPUåŠ é€Ÿå·²é…ç½®ï¼")
        print(f"é¢„è®¡ç”Ÿæˆé€Ÿåº¦: {settings.get('estimated_speed', 'æœªçŸ¥')}")
        print(f"ç›¸æ¯”CPUæ¨¡å¼æå‡: 5-10å€")
        
        restart_service_with_gpu()
    else:
        print(f"\nâš ï¸  æ— æ³•å¯ç”¨GPUåŠ é€Ÿ")
        print("è¯·æ£€æŸ¥:")
        print("1. æ˜¯å¦å®‰è£…äº†æ”¯æŒCUDAçš„PyTorch")
        print("2. NVIDIAé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("3. GPUæ˜¯å¦æ”¯æŒCUDA")

if __name__ == "__main__":
    main() 